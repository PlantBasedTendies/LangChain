{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d610b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda94076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 696, which is longer than the specified 600\n",
      "Created a chunk of size 1186, which is longer than the specified 600\n",
      "Created a chunk of size 1289, which is longer than the specified 600\n",
      "Created a chunk of size 988, which is longer than the specified 600\n",
      "Created a chunk of size 1452, which is longer than the specified 600\n",
      "Created a chunk of size 1030, which is longer than the specified 600\n",
      "Created a chunk of size 887, which is longer than the specified 600\n",
      "Created a chunk of size 1022, which is longer than the specified 600\n",
      "Created a chunk of size 1012, which is longer than the specified 600\n",
      "Created a chunk of size 604, which is longer than the specified 600\n",
      "Created a chunk of size 1193, which is longer than the specified 600\n",
      "Created a chunk of size 1015, which is longer than the specified 600\n",
      "Created a chunk of size 927, which is longer than the specified 600\n",
      "Created a chunk of size 630, which is longer than the specified 600\n",
      "Created a chunk of size 785, which is longer than the specified 600\n",
      "Created a chunk of size 795, which is longer than the specified 600\n",
      "Created a chunk of size 760, which is longer than the specified 600\n",
      "Created a chunk of size 893, which is longer than the specified 600\n",
      "Created a chunk of size 706, which is longer than the specified 600\n",
      "Created a chunk of size 603, which is longer than the specified 600\n",
      "Created a chunk of size 653, which is longer than the specified 600\n",
      "Created a chunk of size 815, which is longer than the specified 600\n",
      "Created a chunk of size 689, which is longer than the specified 600\n",
      "Created a chunk of size 709, which is longer than the specified 600\n",
      "Created a chunk of size 680, which is longer than the specified 600\n",
      "Created a chunk of size 604, which is longer than the specified 600\n",
      "Created a chunk of size 742, which is longer than the specified 600\n",
      "Created a chunk of size 1014, which is longer than the specified 600\n",
      "Created a chunk of size 602, which is longer than the specified 600\n",
      "Created a chunk of size 1476, which is longer than the specified 600\n",
      "Created a chunk of size 655, which is longer than the specified 600\n",
      "Created a chunk of size 952, which is longer than the specified 600\n",
      "Created a chunk of size 913, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 1321, which is longer than the specified 600\n",
      "Created a chunk of size 799, which is longer than the specified 600\n",
      "Created a chunk of size 653, which is longer than the specified 600\n",
      "Created a chunk of size 810, which is longer than the specified 600\n",
      "Created a chunk of size 699, which is longer than the specified 600\n",
      "Created a chunk of size 754, which is longer than the specified 600\n",
      "Created a chunk of size 608, which is longer than the specified 600\n",
      "Created a chunk of size 905, which is longer than the specified 600\n",
      "Created a chunk of size 993, which is longer than the specified 600\n",
      "Created a chunk of size 621, which is longer than the specified 600\n",
      "Created a chunk of size 741, which is longer than the specified 600\n",
      "Created a chunk of size 1184, which is longer than the specified 600\n",
      "Created a chunk of size 1209, which is longer than the specified 600\n",
      "Created a chunk of size 854, which is longer than the specified 600\n",
      "Created a chunk of size 791, which is longer than the specified 600\n",
      "Created a chunk of size 1003, which is longer than the specified 600\n",
      "Created a chunk of size 685, which is longer than the specified 600\n",
      "Created a chunk of size 608, which is longer than the specified 600\n",
      "Created a chunk of size 1032, which is longer than the specified 600\n",
      "Created a chunk of size 1081, which is longer than the specified 600\n",
      "Created a chunk of size 689, which is longer than the specified 600\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load document with LangChain's TextLoader\n",
    "    \n",
    "loader = TextLoader('./AI_executive_order_oct_2023.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk the document\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2758dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"langchain_a0689d8fcff84e3094cefc45718e2a8a_PyYEqHYtcKi2\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:06-05:00\",\"took\":34405}\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and vector store\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client = client,\n",
    "    documents = chunks,\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    by_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6069de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you do not know.\\nUse three setences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\", template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Augment\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you do not know.\n",
    "Use three setences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cca0fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Shutting down... \",\"time\":\"2023-12-19T13:30:14-05:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Stopped serving weaviate at http://127.0.0.1:6666\",\"time\":\"2023-12-19T13:30:14-05:00\"}\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser ()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92aa3307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded weaviate wasn't listening on port 6666, so starting embedded weaviate again\n",
      "Started /home/mg/.cache/weaviate-embedded: process ID 522478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2023-12-19T13:30:18-05:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2023-12-19T13:30:18-05:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_2f4e7d761bcd4f42ad68821c5a74e599_mqvV0kRdIYTu\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:18-05:00\",\"took\":4589711}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_8106329845ba457f878de3b20e80eb6d_0CrCTCTejrCQ\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:18-05:00\",\"took\":3063230}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_a53b0cd7f66447a89f8e3034832a2da0_EBQbQoTLmTD2\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:18-05:00\",\"took\":1333366}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_a0689d8fcff84e3094cefc45718e2a8a_PyYEqHYtcKi2\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:18-05:00\",\"took\":2259242}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_abba61155ca7470aa6a7dde44b02bce1_Y00Y38WQNFBt\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:18-05:00\",\"took\":3448287}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_b7c12b2062bd45f18ec1d63b693e7a6d_WkVpYqXRf4R3\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:18-05:00\",\"took\":1204803}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2023-12-19T13:30:18-05:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50051\",\"time\":\"2023-12-19T13:30:18-05:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_d896435385214955ab9147dc76eb446a_TtcQ5EuPYyXV\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-19T13:30:18-05:00\",\"took\":4967833}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:6666\",\"time\":\"2023-12-19T13:30:18-05:00\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Secretary of Homeland Security will establish an Artificial Intelligence Safety and Security Board as an advisory committee. The board will include AI experts from the private sector, academia, and government. Its purpose is to provide advice, information, or recommendations for improving security, resilience, and incident response related to AI usage in critical infrastructure.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[0]:\n",
    "\n",
    "query = \"How will the Secretary of Homeland Security create an AI Safety Board?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The Secretary of Homeland Security will establish an Artificial Intelligence\n",
    "# Safety and Security Board as an advisory committee. The board will include AI experts\n",
    "# from the private sector, academia, and government. Its purpose is to provide advice,\n",
    "# information, or recommendations for improving security, resilience, and incident response\n",
    "# related to AI usage in critical infrastructure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbb04ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AI Executive Order defines watermarking as the act of embedding information into AI outputs for the purpose of verifying authenticity, identity, modifications, or conveyance.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[1]:\n",
    "\n",
    "query = \"What does the AI Executive Order say about watermarking?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The AI Executive Order defines watermarking as the act of\n",
    "# embedding difficult-to-remove information into AI outputs for the\n",
    "# purpose of verifying authenticity, identity, modifications, or conveyance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24923487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The theoretical maximum computing capacity cutoff for the technical conditions for AI models is 1020 integer or floating-point operations per second for training AI.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[2]:\n",
    "\n",
    "query = \"What is the theoretical maximum computing capacity cutoff for the technical conditions for AI models?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The theoretical maximum computing capacity cutoff for the technical conditions for AI models is 1020\n",
    "# integer or floating-point operations per second for training AI.\"\n",
    "#\n",
    "# **Note: 1020 means 10^20, and is correct based on the original text document which omits the '^' sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e9c501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know the answer.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[3]:\n",
    "\n",
    "query = \"What is the easiest way to time travel?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"I do not know the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa39013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928a436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6a3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748036f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c274a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a2b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5685d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6dd227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 Jupyter Env",
   "language": "python",
   "name": "joop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
