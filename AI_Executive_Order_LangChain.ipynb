{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d610b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda94076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 696, which is longer than the specified 500\n",
      "Created a chunk of size 572, which is longer than the specified 500\n",
      "Created a chunk of size 1186, which is longer than the specified 500\n",
      "Created a chunk of size 1289, which is longer than the specified 500\n",
      "Created a chunk of size 988, which is longer than the specified 500\n",
      "Created a chunk of size 1452, which is longer than the specified 500\n",
      "Created a chunk of size 1030, which is longer than the specified 500\n",
      "Created a chunk of size 887, which is longer than the specified 500\n",
      "Created a chunk of size 1022, which is longer than the specified 500\n",
      "Created a chunk of size 1012, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 533, which is longer than the specified 500\n",
      "Created a chunk of size 604, which is longer than the specified 500\n",
      "Created a chunk of size 514, which is longer than the specified 500\n",
      "Created a chunk of size 1193, which is longer than the specified 500\n",
      "Created a chunk of size 1015, which is longer than the specified 500\n",
      "Created a chunk of size 521, which is longer than the specified 500\n",
      "Created a chunk of size 927, which is longer than the specified 500\n",
      "Created a chunk of size 630, which is longer than the specified 500\n",
      "Created a chunk of size 785, which is longer than the specified 500\n",
      "Created a chunk of size 795, which is longer than the specified 500\n",
      "Created a chunk of size 760, which is longer than the specified 500\n",
      "Created a chunk of size 566, which is longer than the specified 500\n",
      "Created a chunk of size 893, which is longer than the specified 500\n",
      "Created a chunk of size 706, which is longer than the specified 500\n",
      "Created a chunk of size 603, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 653, which is longer than the specified 500\n",
      "Created a chunk of size 536, which is longer than the specified 500\n",
      "Created a chunk of size 561, which is longer than the specified 500\n",
      "Created a chunk of size 815, which is longer than the specified 500\n",
      "Created a chunk of size 570, which is longer than the specified 500\n",
      "Created a chunk of size 689, which is longer than the specified 500\n",
      "Created a chunk of size 536, which is longer than the specified 500\n",
      "Created a chunk of size 709, which is longer than the specified 500\n",
      "Created a chunk of size 680, which is longer than the specified 500\n",
      "Created a chunk of size 604, which is longer than the specified 500\n",
      "Created a chunk of size 742, which is longer than the specified 500\n",
      "Created a chunk of size 528, which is longer than the specified 500\n",
      "Created a chunk of size 1014, which is longer than the specified 500\n",
      "Created a chunk of size 602, which is longer than the specified 500\n",
      "Created a chunk of size 505, which is longer than the specified 500\n",
      "Created a chunk of size 1476, which is longer than the specified 500\n",
      "Created a chunk of size 655, which is longer than the specified 500\n",
      "Created a chunk of size 539, which is longer than the specified 500\n",
      "Created a chunk of size 533, which is longer than the specified 500\n",
      "Created a chunk of size 553, which is longer than the specified 500\n",
      "Created a chunk of size 952, which is longer than the specified 500\n",
      "Created a chunk of size 550, which is longer than the specified 500\n",
      "Created a chunk of size 913, which is longer than the specified 500\n",
      "Created a chunk of size 700, which is longer than the specified 500\n",
      "Created a chunk of size 587, which is longer than the specified 500\n",
      "Created a chunk of size 1321, which is longer than the specified 500\n",
      "Created a chunk of size 799, which is longer than the specified 500\n",
      "Created a chunk of size 653, which is longer than the specified 500\n",
      "Created a chunk of size 810, which is longer than the specified 500\n",
      "Created a chunk of size 699, which is longer than the specified 500\n",
      "Created a chunk of size 754, which is longer than the specified 500\n",
      "Created a chunk of size 608, which is longer than the specified 500\n",
      "Created a chunk of size 905, which is longer than the specified 500\n",
      "Created a chunk of size 521, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 546, which is longer than the specified 500\n",
      "Created a chunk of size 993, which is longer than the specified 500\n",
      "Created a chunk of size 577, which is longer than the specified 500\n",
      "Created a chunk of size 621, which is longer than the specified 500\n",
      "Created a chunk of size 579, which is longer than the specified 500\n",
      "Created a chunk of size 741, which is longer than the specified 500\n",
      "Created a chunk of size 513, which is longer than the specified 500\n",
      "Created a chunk of size 1184, which is longer than the specified 500\n",
      "Created a chunk of size 1209, which is longer than the specified 500\n",
      "Created a chunk of size 854, which is longer than the specified 500\n",
      "Created a chunk of size 791, which is longer than the specified 500\n",
      "Created a chunk of size 1003, which is longer than the specified 500\n",
      "Created a chunk of size 685, which is longer than the specified 500\n",
      "Created a chunk of size 608, which is longer than the specified 500\n",
      "Created a chunk of size 1032, which is longer than the specified 500\n",
      "Created a chunk of size 1081, which is longer than the specified 500\n",
      "Created a chunk of size 548, which is longer than the specified 500\n",
      "Created a chunk of size 579, which is longer than the specified 500\n",
      "Created a chunk of size 689, which is longer than the specified 500\n",
      "Created a chunk of size 546, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load document with LangChain's TextLoader\n",
    "    \n",
    "loader = TextLoader('./AI_executive_order_oct_2023.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk the document\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2758dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 6666\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and vector store\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client = client,\n",
    "    documents = chunks,\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    by_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6069de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you do not know.\\nUse three setences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\", template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Augment\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you do not know.\n",
    "Use three setences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca0fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser ()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92aa3307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Secretary of Homeland Security will establish an Artificial Intelligence Safety and Security Board as an advisory committee. The board will include AI experts from the private sector, academia, and government. Its purpose is to provide advice, information, or recommendations for improving security, resilience, and incident response related to AI usage in critical infrastructure.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[0]:\n",
    "\n",
    "query = \"How will the Secretary of Homeland Security create an AI Safety Board?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The Secretary of Homeland Security will establish an Artificial Intelligence\n",
    "# Safety and Security Board as an advisory committee. The board will include AI experts\n",
    "# from the private sector, academia, and government. Its purpose is to provide advice,\n",
    "# information, or recommendations for improving security, resilience, and incident response\n",
    "# related to AI usage in critical infrastructure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cbb04ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AI Executive Order defines watermarking as the act of embedding difficult-to-remove information into AI outputs for the purpose of verifying authenticity, identity, modifications, or conveyance.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[1]:\n",
    "\n",
    "query = \"What does the AI Executive Order say about watermarking?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The AI Executive Order defines watermarking as the act of\n",
    "# embedding difficult-to-remove information into AI outputs for the\n",
    "# purpose of verifying authenticity, identity, modifications, or conveyance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24923487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The theoretical maximum computing capacity cutoff for the technical conditions for AI models is 1020 integer or floating-point operations per second for training AI.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[2]:\n",
    "\n",
    "query = \"What is the theoretical maximum computing capacity cutoff for the technical conditions for AI models?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The theoretical maximum computing capacity cutoff for the technical conditions for AI models is 1020\n",
    "# integer or floating-point operations per second for training AI.\"\n",
    "#\n",
    "# **Note: 1020 means 10^20, and is correct based on the original text document which omits the '^' sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9c501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know the answer.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[2]:\n",
    "\n",
    "query = \"What is the easiest way to time travel?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"I do not know the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa39013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928a436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6a3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748036f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c274a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a2b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5685d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6dd227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 Jupyter Env",
   "language": "python",
   "name": "joop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
