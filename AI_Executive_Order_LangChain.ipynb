{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d610b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fda94076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function: load_doc\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextLoader' object has no attribute 'loadX'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mloadX()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n\u001b[0;32m---> 19\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mload_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Chunk the document\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@log_function\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunking\u001b[39m():\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mlog_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36mload_doc\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@log_function\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_doc\u001b[39m():\n\u001b[1;32m     15\u001b[0m     loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./AI_executive_order_oct_2023.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadX\u001b[49m()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextLoader' object has no attribute 'loadX'"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Create a decorator that logs function before executing it for debugging:\n",
    "\n",
    "def log_function(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"Calling function: {func.__name__}\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# Load document with LangChain's TextLoader\n",
    "\n",
    "@log_function\n",
    "def load_doc():\n",
    "    loader = TextLoader('./AI_executive_order_oct_2023.txt')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_doc()\n",
    "\n",
    "# Chunk the document\n",
    "\n",
    "@log_function\n",
    "def chunking():\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "chunks = chunking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2758dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function: embed_and_vector\n",
      "embedded weaviate is already listening on port 6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"langchain_e50711c8f7814d73bfa62fd902e164be_eQT15VTliFkA\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-12-20T10:19:56-05:00\",\"took\":34996}\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and vector store\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "@log_function\n",
    "def embed_and_vector():\n",
    "    client = weaviate.Client(\n",
    "        embedded_options = EmbeddedOptions()\n",
    "    )\n",
    "\n",
    "    vectorstore = Weaviate.from_documents(\n",
    "        client = client,\n",
    "        documents = chunks,\n",
    "        embedding = OpenAIEmbeddings(),\n",
    "        by_text = False\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = embed_and_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6069de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you do not know.\\nUse three setences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\", template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Augment\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you do not know.\n",
    "Use three setences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cca0fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser ()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92aa3307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Secretary of Homeland Security will establish an Artificial Intelligence Safety and Security Board as an advisory committee. The board will include AI experts from the private sector, academia, and government. Its purpose is to provide advice, information, or recommendations for improving security, resilience, and incident response related to AI usage in critical infrastructure.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query[0]:\n",
    "\n",
    "query = \"How will the Secretary of Homeland Security create an AI Safety Board?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The Secretary of Homeland Security will establish an Artificial Intelligence\n",
    "# Safety and Security Board as an advisory committee. The board will include AI experts\n",
    "# from the private sector, academia, and government. Its purpose is to provide advice,\n",
    "# information, or recommendations for improving security, resilience, and incident response\n",
    "# related to AI usage in critical infrastructure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query[1]:\n",
    "\n",
    "query = \"What does the AI Executive Order say about watermarking?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The AI Executive Order defines watermarking as the act of\n",
    "# embedding difficult-to-remove information into AI outputs for the\n",
    "# purpose of verifying authenticity, identity, modifications, or conveyance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24923487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query[2]:\n",
    "\n",
    "query = \"What is the theoretical maximum computing capacity cutoff for the technical conditions for AI models?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"The theoretical maximum computing capacity cutoff for the technical conditions for AI models is 1020\n",
    "# integer or floating-point operations per second for training AI.\"\n",
    "#\n",
    "# **Note: 1020 means 10^20, and is correct based on the original text document which omits the '^' sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query[3]:\n",
    "\n",
    "query = \"What is the easiest way to time travel?\"\n",
    "rag_chain.invoke(query)\n",
    "\n",
    "# Output: \"I do not know the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa39013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928a436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6a3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748036f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c274a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a2b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5685d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6dd227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 Jupyter Env",
   "language": "python",
   "name": "joop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
